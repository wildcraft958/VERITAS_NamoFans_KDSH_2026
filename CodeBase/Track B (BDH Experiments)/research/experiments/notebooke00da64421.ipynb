{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14438742,"sourceType":"datasetVersion","datasetId":9222701},{"sourceId":14438759,"sourceType":"datasetVersion","datasetId":9222713}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# CELL 0: ULTIMATE PATCH (Ver 3) - RESTART KERNEL FIRST\n# ============================================\nimport sys\nimport os\nimport shutil\n\n# Remove old paths\nsys.path = [p for p in sys.path if 'bdh-code' not in p.lower()]\n\nCODE_SRC = '/kaggle/input/bdh-code/BDH'\nCODE_DST = '/kaggle/working/BDH'\n\n# Fresh copy\nif os.path.exists(CODE_DST):\n    shutil.rmtree(CODE_DST)\nshutil.copytree(CODE_SRC, CODE_DST)\n\n# 1. Patch backstory_parser.py\nbp_path = os.path.join(CODE_DST, 'narrative_reasoning', 'backstory_parser.py')\nwith open(bp_path, 'r') as f:\n    content = f.read()\ncontent = content.replace('import re\\n', 'import re\\nimport torch\\n')\nwith open(bp_path, 'w') as f:\n    f.write(content)\n\n# 2. Patch modeling_bdh.py (Added missing get_state/update_state)\nmbdh_path = os.path.join(CODE_DST, 'master_bdh', 'modeling_bdh.py')\nwith open(mbdh_path, 'r') as f:\n    lines = f.readlines()\n\nnew_lines = []\nskip = False\nfor line in lines:\n    if 'from transformers.cache_utils import Cache, CacheLayerMixin' in line:\n        new_lines.append('from transformers.cache_utils import Cache\\n')\n        new_lines.append('try:\\n    from transformers.cache_utils import CacheLayerMixin\\nexcept ImportError:\\n    class CacheLayerMixin: pass\\n')\n        continue\n    if 'class BDHCache(Cache):' in line:\n        skip = True\n        new_lines.append('''class BDHCache(Cache):\n    def __init__(self, config, max_batch_size=None, max_cache_len=None, device=None, dtype=None):\n        try: super().__init__()\n        except: pass\n        self.layers = []\n        self.config = config\n        self.dtype = dtype or torch.float32\n        self._max_batch_size = max_batch_size\n        self._max_cache_len = max_cache_len\n        self._seen_tokens = 0\n    def update(self, key_states, value_states, layer_idx, cache_kwargs=None):\n        while len(self.layers) <= layer_idx: self.layers.append(BDHCacheLayer())\n        if layer_idx == 0: self._seen_tokens += key_states.shape[-2]\n        return key_states, value_states\n    def get_seq_length(self, layer_idx=0): return self._seen_tokens\n    def detach_(self):\n        for layer in self.layers:\n            if hasattr(layer, 'recurrent_state') and layer.recurrent_state is not None:\n                layer.recurrent_state = layer.recurrent_state.detach()\n    def update_state(self, layer_idx, new_state):\n        while len(self.layers) <= layer_idx: self.layers.append(BDHCacheLayer())\n        self.layers[layer_idx].recurrent_state = new_state.to(torch.float32)\n    def get_state(self, layer_idx):\n        if layer_idx < len(self.layers): return self.layers[layer_idx].recurrent_state\n        return None\n    @property\n    def max_batch_size(self): return self._max_batch_size\n    @property\n    def max_cache_len(self): return self._max_cache_len\n''')\n        continue\n    if skip and (line.startswith('class ') or line.startswith('# Note')):\n        skip = False\n    if not skip: new_lines.append(line)\n\nwith open(mbdh_path, 'w') as f:\n    f.writelines(new_lines)\n\n# 3. Patch train_pipeline.py (Full file with Dataset class & OOM fix)\ntp_path = os.path.join(CODE_DST, 'training', 'train_pipeline.py')\nnew_tp_content = r'''\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Dict, Optional, List, Tuple\nimport os\nfrom tqdm import tqdm\nimport gc\n\nfrom master_bdh import BDHConfig, BDHForCausalLM\nfrom narrative_reasoning.consistency_classifier import ConsistencyClassifier\nfrom narrative_reasoning.representation_extractor import RepresentationExtractor\nfrom narrative_reasoning.backstory_parser import BackstoryParser, BackstoryEmbedder\nfrom training.pretrain_bdh import pretrain_bdh, NarrativeDataset\n\nclass ConsistencyDataset(Dataset):\n    def __init__(self, narratives, backstories, labels, tokenizer, narrative_processor, backstory_parser, max_narrative_length=2048):\n        self.narratives = narratives\n        self.backstories = backstories\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.narrative_processor = narrative_processor\n        self.backstory_parser = backstory_parser\n        self.max_narrative_length = max_narrative_length\n        self.parsed_backstories = [self.backstory_parser.parse(b) for b in backstories]\n    def __len__(self): return len(self.narratives)\n    def __getitem__(self, idx):\n        return {'narrative': self.narratives[idx], 'backstory': self.backstories[idx], \n                'backstory_claims': self.parsed_backstories[idx], 'label': self.labels[idx]}\n\ndef train_consistency_classifier(\n    model, classifier, train_dataset, val_dataset=None,\n    num_epochs=10, batch_size=4, learning_rate=1e-4, weight_decay=0.01,\n    device=None, save_dir=None, use_bf16=True, freeze_bdh=True,\n    tokenizer=None, narrative_processor=None\n):\n    if device is None: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    torch.cuda.empty_cache(); gc.collect()\n    \n    model = model.to(device).eval()\n    classifier = classifier.to(device).train()\n    \n    if freeze_bdh:\n        for p in model.parameters(): p.requires_grad = False\n            \n    base = train_dataset.dataset if hasattr(train_dataset, 'dataset') else train_dataset\n    tokenizer = tokenizer or getattr(base, 'tokenizer', None)\n    narrative_processor = narrative_processor or getattr(base, 'narrative_processor', None)\n    \n    def collate_fn(batch):\n        return {\n            'narrative': [b['narrative'] for b in batch],\n            'backstory_claims': [b['backstory_claims'] for b in batch],\n            'label': torch.tensor([b['label'] for b in batch], dtype=torch.long)\n        }\n\n    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size, False, collate_fn=collate_fn, num_workers=0) if val_dataset else None\n    \n    optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = nn.CrossEntropyLoss()\n    backstory_embedder = BackstoryEmbedder(tokenizer, model, device)\n    \n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        classifier.train()\n        total_loss, correct, total = 0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        \n        for batch in pbar:\n            labels = batch['label'].to(device)\n            narr_reprs = []\n            \n            for narr in batch['narrative']:\n                chunks, _ = narrative_processor.process_narrative(narr)\n                if len(chunks)>200: chunks = chunks[:200]\n                \n                c_reps = []\n                for c in chunks:\n                    c = c.to(device)\n                    if c.shape[-1]>2048: c = c[:, :2048]\n                    with torch.no_grad():\n                        out = model(input_ids=c, output_hidden_states=True)\n                        c_reps.append(out.hidden_states[-1].mean(1).detach().cpu())\n                    del out, c\n                    torch.cuda.empty_cache()\n                \n                if c_reps: narr_reprs.append(torch.stack(c_reps).mean(0).to(device))\n                else: narr_reprs.append(torch.zeros(model.config.hidden_size).to(device))\n            \n            back_reprs = [backstory_embedder.aggregate_claims(c) for c in batch['backstory_claims']]\n            \n            # Forward\n            logits, _ = classifier(torch.stack(narr_reprs), torch.stack(back_reprs))\n            loss = criterion(logits, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            correct += (logits.argmax(1) == labels).sum().item()\n            total += len(labels)\n            \n            del logits, loss, narr_reprs, back_reprs\n            pbar.set_postfix({'acc': correct/total})\n            \n        # Validation\n        if val_loader:\n            classifier.eval()\n            v_correct, v_total = 0, 0\n            with torch.no_grad():\n                for batch in tqdm(val_loader, desc=\"Val\"):\n                    labels = batch['label'].to(device)\n                    narr_reprs = []\n                    for narr in batch['narrative']:\n                        chunks, _ = narrative_processor.process_narrative(narr)\n                        if len(chunks)>200: chunks = chunks[:200]\n                        c_reps = [model(input_ids=c.to(device)[:,:2048], output_hidden_states=True).hidden_states[-1].mean(1).detach().cpu() for c in chunks]\n                        narr_reprs.append(torch.stack(c_reps).mean(0).to(device) if c_reps else torch.zeros(model.config.hidden_size).to(device))\n                    \n                    back_reprs = [backstory_embedder.aggregate_claims(c) for c in batch['backstory_claims']]\n                    logits, _ = classifier(torch.stack(narr_reprs), torch.stack(back_reprs))\n                    v_correct += (logits.argmax(1)==labels).sum().item()\n                    v_total += len(labels)\n            \n            val_acc = v_correct/v_total\n            print(f\"Epoch {epoch+1}: Train Acc={correct/total:.3f}, Val Acc={val_acc:.3f}\")\n            if val_acc > best_acc and save_dir:\n                best_acc = val_acc\n                os.makedirs(save_dir, exist_ok=True)\n                torch.save(classifier.state_dict(), os.path.join(save_dir, 'best_classifier.pt'))\n                \n    return classifier, model\n'''\nwith open(tp_path, 'w') as f:\n    f.write(new_tp_content)\n\nsys.path.insert(0, CODE_DST)\nprint(\"‚úÖ All patches applied successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:25.753285Z","iopub.execute_input":"2026-01-08T22:59:25.753749Z","iopub.status.idle":"2026-01-08T22:59:26.413021Z","shell.execute_reply.started":"2026-01-08T22:59:25.753734Z","shell.execute_reply":"2026-01-08T22:59:26.412538Z"}},"outputs":[{"name":"stdout","text":"‚úÖ All patches applied successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================\n# CELL 1: Setup and GPU Verification\n# ============================================\n\nimport os\nimport sys\nimport gc\nimport torch\nimport pandas as pd\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable H100 optimizations\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\n# Verify GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"=\" * 60)\nprint(\"GPU VERIFICATION\")\nprint(\"=\" * 60)\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    print(f\"bfloat16 Supported: {torch.cuda.is_bf16_supported()}\")\nelse:\n    print(\"WARNING: Running on CPU - will be very slow!\")\n\n# Set directories\nWORK_DIR = Path(\"/kaggle/working\")\nINPUT_DIR = Path(\"/kaggle/input\")\nCHECKPOINT_DIR = WORK_DIR / \"checkpoints\"\nCHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n\nprint(f\"\\nWorking directory: {WORK_DIR}\")\nprint(f\"Input directory: {INPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:45.575870Z","iopub.execute_input":"2026-01-08T22:59:45.576389Z","iopub.status.idle":"2026-01-08T22:59:45.581991Z","shell.execute_reply.started":"2026-01-08T22:59:45.576373Z","shell.execute_reply":"2026-01-08T22:59:45.581506Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU VERIFICATION\n============================================================\nDevice: cuda\nGPU: NVIDIA H100 80GB HBM3\nMemory: 85.3 GB\nCUDA Version: 12.4\nbfloat16 Supported: True\n\nWorking directory: /kaggle/working\nInput directory: /kaggle/input\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -------------------------------------------------\n# HARD GPU STATE RESET\n# Purpose:\n#   Ensure a clean GPU memory slate before\n#   re-instantiating large models.\n#\n# Why this matters:\n#   - Python keeps references alive longer than expected\n#   - PyTorch does not immediately return memory to CUDA\n#   - Reinitializing models without cleanup ‚Üí silent OOMs\n#\n# What this block does:\n#   1. Explicitly deletes existing model objects (if present)\n#   2. Flushes PyTorch's CUDA memory cache\n#   3. Forces Python garbage collection to reclaim orphaned tensors\n# -------------------------------------------------\n\n# Remove existing model instance from global scope, if it exists\nif 'model' in globals():\n    print(\"üßπ Deleting existing model instance from memory\")\n    del model\n\n# Remove existing classifier instance from global scope, if it exists\nif 'classifier' in globals():\n    print(\"üßπ Deleting existing classifier instance from memory\")\n    del classifier\n\n# Release all cached CUDA memory held by PyTorch\nprint(\"üßπ Clearing CUDA memory cache\")\ntorch.cuda.empty_cache()\n\n# Force garbage collection to clean up dangling Python references\nprint(\"üßπ Running Python garbage collection\")\ngc.collect()\n\nprint(\"‚úÖ GPU memory reset complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:46.710346Z","iopub.execute_input":"2026-01-08T22:59:46.710946Z","iopub.status.idle":"2026-01-08T22:59:46.972500Z","shell.execute_reply.started":"2026-01-08T22:59:46.710930Z","shell.execute_reply":"2026-01-08T22:59:46.972030Z"}},"outputs":[{"name":"stdout","text":"üßπ Clearing CUDA memory cache\nüßπ Running Python garbage collection\n‚úÖ GPU memory reset complete\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# # ============================================\n# # CELL 2: Install Dependencies\n# # ============================================\n\n# # Uncomment these in Kaggle:\n# !pip install -q transformers datasets accelerate\n# !pip install -q scikit-learn matplotlib seaborn networkx\n\n# import transformers\n# print(f\"Transformers version: {transformers.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:50.363815Z","iopub.execute_input":"2026-01-08T22:59:50.364337Z","iopub.status.idle":"2026-01-08T22:59:50.366644Z","shell.execute_reply.started":"2026-01-08T22:59:50.364321Z","shell.execute_reply":"2026-01-08T22:59:50.366200Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# CELL 3: Add Project to Path & Import\n# ============================================\nimport sys\n# For Kaggle, assuming code is uploaded as a dataset\n# Adjust this path based on your dataset name\nCODE_DATASET_NAME = \"bdh-code/BDH\"  # CHANGE THIS!\nDATA_DATASET_NAME = \"kdsh-data/Dataset_kdsh\"  # CHANGE THIS!\n\ncode_path = INPUT_DIR / CODE_DATASET_NAME\ndata_path = INPUT_DIR / DATA_DATASET_NAME\n\n# Add code to Python path\nif code_path.exists():\n    sys.path.insert(0, str(code_path))\n    print(f\"Added {code_path} to Python path\")\nelse:\n    # Fallback: code might be in working directory\n    sys.path.insert(0, str(WORK_DIR))\n    print(f\"Code dataset not found at {code_path}\")\n    print(\"Please ensure your code is uploaded as a Kaggle dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:51.675327Z","iopub.execute_input":"2026-01-08T22:59:51.675548Z","iopub.status.idle":"2026-01-08T22:59:51.679494Z","shell.execute_reply.started":"2026-01-08T22:59:51.675535Z","shell.execute_reply":"2026-01-08T22:59:51.679046Z"}},"outputs":[{"name":"stdout","text":"Added /kaggle/input/bdh-code/BDH to Python path\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================\n# CELL 4: Load Dataset\n# ============================================\n\ndef load_and_prepare_data(data_dir: Path):\n    \"\"\"Load train/test CSVs and Books directory.\"\"\"\n    \n    # Try to find the data\n    possible_dirs = [\n        data_dir,\n        data_dir / \"BDH\",\n        INPUT_DIR,\n    ]\n    \n    train_df = None\n    test_df = None\n    books_dir = None\n    \n    for d in possible_dirs:\n        train_path = d / \"train.csv\"\n        test_path = d / \"test.csv\"\n        books_path = d / \"Books\"\n        \n        if train_path.exists():\n            train_df = pd.read_csv(train_path)\n            print(f\"Found train.csv at {train_path}\")\n        if test_path.exists():\n            test_df = pd.read_csv(test_path)\n            print(f\"Found test.csv at {test_path}\")\n        if books_path.exists():\n            books_dir = books_path\n            print(f\"Found Books/ at {books_path}\")\n    \n    return train_df, test_df, books_dir\n\n# Load data\ntrain_df, test_df, books_dir = load_and_prepare_data(data_path)\n\nif train_df is not None:\n    print(f\"\\nTrain samples: {len(train_df)}\")\n    print(f\"Columns: {train_df.columns.tolist()}\")\n    print(\"\\nSample row:\")\n    print(train_df.head(1).to_dict('records')[0])\n\nif test_df is not None:\n    print(f\"\\nTest samples: {len(test_df)}\")\n\nif books_dir:\n    book_files = list(books_dir.glob(\"*.txt\"))\n    print(f\"\\nBooks available: {len(book_files)}\")\n    print(f\"Sample books: {[f.stem for f in book_files[:5]]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:53.305653Z","iopub.execute_input":"2026-01-08T22:59:53.305914Z","iopub.status.idle":"2026-01-08T22:59:53.325147Z","shell.execute_reply.started":"2026-01-08T22:59:53.305898Z","shell.execute_reply":"2026-01-08T22:59:53.324695Z"}},"outputs":[{"name":"stdout","text":"Found train.csv at /kaggle/input/kdsh-data/Dataset_kdsh/train.csv\nFound test.csv at /kaggle/input/kdsh-data/Dataset_kdsh/test.csv\nFound Books/ at /kaggle/input/kdsh-data/Dataset_kdsh/Books\n\nTrain samples: 80\nColumns: ['id', 'book_name', 'char', 'caption', 'content', 'label']\n\nSample row:\n{'id': 46, 'book_name': 'In Search of the Castaways', 'char': 'Thalcave', 'caption': nan, 'content': 'Thalcave‚Äôs people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.', 'label': 'consistent'}\n\nTest samples: 60\n\nBooks available: 2\nSample books: ['In search of the castaways', 'The Count of Monte Cristo']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================\n# CELL 5: Import BDH Components\n# ============================================\n\ntry:\n    from master_bdh import BDHConfig, BDHForCausalLM\n    from master_bdh.continual_learning import ContinualLearningWrapper\n    from narrative_reasoning.narrative_processor import NarrativeProcessor\n    from narrative_reasoning.backstory_parser import BackstoryParser, BackstoryEmbedder\n    from narrative_reasoning.representation_extractor import RepresentationExtractor\n    from narrative_reasoning.consistency_classifier import ConsistencyClassifier\n    from training.pretrain_bdh import pretrain_bdh, NarrativeDataset\n    from training.train_pipeline import train_consistency_classifier, ConsistencyDataset\n    from utils.data_loader import (\n        load_dataset_from_path,\n        prepare_training_data,\n        prepare_test_data\n    )\n    print(\"‚úÖ All BDH components imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ùå Import error: {e}\")\n    print(\"Make sure code dataset path is correct\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T22:59:30.620951Z","iopub.execute_input":"2026-01-08T22:59:30.621343Z","iopub.status.idle":"2026-01-08T22:59:37.803292Z","shell.execute_reply.started":"2026-01-08T22:59:30.621327Z","shell.execute_reply":"2026-01-08T22:59:37.802771Z"}},"outputs":[{"name":"stderr","text":"2026-01-08 22:59:34.211487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767913174.226552    2983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767913174.231071    2983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"‚úÖ All BDH components imported successfully\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================\n# CELL 6: Initialize Model and Tokenizer (FIXED)\n# ============================================\n\nimport torch\nimport gc\nfrom transformers import AutoTokenizer\nfrom master_bdh import BDHConfig, BDHForCausalLM\nfrom narrative_reasoning.consistency_classifier import ConsistencyClassifier\n\n# ----------------------------\n# Hard reset GPU state\n# ----------------------------\nif 'model' in globals():\n    del model\nif 'classifier' in globals():\n    del classifier\n\ntorch.cuda.empty_cache()\ngc.collect()\n\n# ----------------------------\n# Lean configuration (OOM-proof)\n# ----------------------------\nCONFIG = {\n    'hidden_size': 256,                 # ‚Üì was 512\n    'num_hidden_layers': 6,             # ‚Üì was 8\n    'num_attention_heads': 4,           # ‚Üì was 8\n    'mlp_internal_dim_multiplier': 64,  # ‚Üì was 256 (primary memory hog)\n    'dropout': 0.1,\n    'max_position_embeddings': 2048,    # realistic context\n    'batch_size': 16,\n    'use_bf16': True,\n}\n\n# ----------------------------\n# Tokenizer\n# ----------------------------\ntokenizer_name = \"bert-base-uncased\"\nprint(f\"Loading tokenizer: {tokenizer_name}\")\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# ----------------------------\n# BDH Model\n# ----------------------------\nconfig = BDHConfig(\n    vocab_size=tokenizer.vocab_size,\n    hidden_size=CONFIG['hidden_size'],\n    num_hidden_layers=CONFIG['num_hidden_layers'],\n    num_attention_heads=CONFIG['num_attention_heads'],\n    mlp_internal_dim_multiplier=CONFIG['mlp_internal_dim_multiplier'],\n    dropout=CONFIG['dropout'],\n    max_position_embeddings=CONFIG['max_position_embeddings'],\n    attn_implementation=\"bdh_recurrent\"  # ‚Üì memory-efficient\n)\n\nprint(\"Initializing BDH model...\")\nmodel = BDHForCausalLM(config)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model parameters: {total_params:,}\")\n\n# ----------------------------\n# Consistency Classifier\n# ----------------------------\nclassifier = ConsistencyClassifier(\n    narrative_dim=CONFIG['hidden_size'],\n    backstory_dim=CONFIG['hidden_size'],\n    hidden_dim=256,\n    num_layers=3,\n    dropout=0.1\n)\n\n# ----------------------------\n# Move to device\n# ----------------------------\nmodel = model.to(device)\nclassifier = classifier.to(device)\n\nprint(f\"‚úÖ Models successfully loaded on {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T23:00:01.588939Z","iopub.execute_input":"2026-01-08T23:00:01.589186Z","iopub.status.idle":"2026-01-08T23:00:02.789145Z","shell.execute_reply.started":"2026-01-08T23:00:01.589171Z","shell.execute_reply":"2026-01-08T23:00:02.788606Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer: bert-base-uncased\nInitializing BDH model...\nModel parameters: 20,396,544\n‚úÖ Models successfully loaded on cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================\n# CELL 7: Prepare Training Data\n# ============================================\n\n# Load narratives from books\nnarratives, backstories, labels = prepare_training_data(\n    train_df, \n    books_dir,\n    verbose=True\n)\n\nprint(f\"\\nNarrative lengths:\")\nprint(f\"  Min: {min(len(n) for n in narratives):,} chars\")\nprint(f\"  Max: {max(len(n) for n in narratives):,} chars\")\nprint(f\"  Avg: {sum(len(n) for n in narratives)//len(narratives):,} chars\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T23:00:05.552390Z","iopub.execute_input":"2026-01-08T23:00:05.552790Z","iopub.status.idle":"2026-01-08T23:00:05.581862Z","shell.execute_reply.started":"2026-01-08T23:00:05.552775Z","shell.execute_reply":"2026-01-08T23:00:05.581385Z"}},"outputs":[{"name":"stdout","text":"Loaded 80 training examples\nUnique books: 2\nLabel distribution: 1=51, 0=29\n\nNarrative lengths:\n  Min: 826,131 chars\n  Max: 2,646,614 chars\n  Avg: 1,531,568 chars\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================\n# CELL 8: Pretrain BDH on Narratives\n# ============================================\n\nfrom torch.utils.data import random_split\n\n# Create pretraining dataset\nprint(\"Creating pretraining dataset...\")\npretrain_dataset = NarrativeDataset(\n    narratives=narratives,\n    tokenizer=tokenizer,\n    max_length=2048\n)\n\n# Split for validation\ntrain_size = int(0.9 * len(pretrain_dataset))\nval_size = len(pretrain_dataset) - train_size\ntrain_ds, val_ds = random_split(pretrain_dataset, [train_size, val_size])\nprint(f\"Pretraining: {len(train_ds)} train, {len(val_ds)} val\")\n\n# Pretrain\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STARTING BDH PRETRAINING\")\nprint(\"=\" * 60)\n\npretrain_bdh(\n    model=model,\n    train_dataset=train_ds,\n    val_dataset=val_ds,\n    num_epochs=3,\n    batch_size=4,\n    learning_rate=1e-4,\n    weight_decay=0.1,\n    grad_accum_steps=4,\n    max_grad_norm=1.0,\n    device=device,\n    save_dir=str(CHECKPOINT_DIR / \"pretrained\"),\n    use_bf16=CONFIG['use_bf16']\n)\nprint(\"‚úÖ Pretraining complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T23:00:07.295312Z","iopub.execute_input":"2026-01-08T23:00:07.295918Z","iopub.status.idle":"2026-01-08T23:01:26.732089Z","shell.execute_reply.started":"2026-01-08T23:00:07.295904Z","shell.execute_reply":"2026-01-08T23:01:26.731546Z"}},"outputs":[{"name":"stdout","text":"Creating pretraining dataset...\nPretraining: 72 train, 8 val\n\n============================================================\nSTARTING BDH PRETRAINING\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:22<00:00,  1.26s/it, loss=9.82]\nValidation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 10.0823, Val Loss = 9.8158\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:23<00:00,  1.28s/it, loss=9.58]\nValidation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 9.7002, Val Loss = 9.5581\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:21<00:00,  1.20s/it, loss=9.35]\nValidation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 9.4662, Val Loss = 9.3354\n‚úÖ Pretraining complete!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================\n# HOTFIX 4: Redefine Training Function\n# ============================================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport os\nimport gc\nfrom narrative_reasoning.backstory_parser import BackstoryEmbedder\n\ndef train_consistency_classifier(\n    model, classifier, train_dataset, val_dataset=None,\n    num_epochs=10, batch_size=4, learning_rate=1e-4, weight_decay=0.01,\n    device=None, save_dir=None, use_bf16=True, freeze_bdh=True,\n    tokenizer=None, narrative_processor=None\n):\n    if device is None: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    torch.cuda.empty_cache(); gc.collect()\n    \n    model = model.to(device).eval()\n    classifier = classifier.to(device).train()\n    \n    if freeze_bdh:\n        for p in model.parameters(): p.requires_grad = False\n            \n    base = train_dataset.dataset if hasattr(train_dataset, 'dataset') else train_dataset\n    tokenizer = tokenizer or getattr(base, 'tokenizer', None)\n    narrative_processor = narrative_processor or getattr(base, 'narrative_processor', None)\n    \n    def collate_fn(batch):\n        return {\n            'narrative': [b['narrative'] for b in batch],\n            'backstory_claims': [b['backstory_claims'] for b in batch],\n            'label': torch.tensor([b['label'] for b in batch], dtype=torch.long)\n        }\n\n    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size, False, collate_fn=collate_fn) if val_dataset else None\n    \n    optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = nn.CrossEntropyLoss()\n    backstory_embedder = BackstoryEmbedder(tokenizer, model, device)\n    \n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        classifier.train()\n        total_loss, correct, total = 0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        \n        for batch in pbar:\n            labels = batch['label'].to(device)\n            narr_reprs = []\n            \n            for narr in batch['narrative']:\n                chunks, _ = narrative_processor.process_narrative(narr)\n                if len(chunks)>200: chunks = chunks[:200]\n                \n                c_reps = []\n                for c in chunks:\n                    c = c.to(device)\n                    if c.shape[-1]>2048: c = c[:, :2048]\n                    with torch.no_grad():\n                        out = model(input_ids=c, output_hidden_states=True)\n                        # FIX: Squeeze the batch dimension [1, D] -> [D]\n                        c_reps.append(out.hidden_states[-1].mean(1).detach().squeeze(0).cpu())\n                    del out, c\n                    torch.cuda.empty_cache()\n                \n                # Stack chunks [N, D] -> Mean [D]\n                if c_reps: narr_reprs.append(torch.stack(c_reps).mean(0).to(device))\n                else: narr_reprs.append(torch.zeros(model.config.hidden_size).to(device))\n            \n            back_reprs = [backstory_embedder.aggregate_claims(c) for c in batch['backstory_claims']]\n            \n            # Forward\n            logits, _ = classifier(torch.stack(narr_reprs), torch.stack(back_reprs))\n            loss = criterion(logits, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            correct += (logits.argmax(1) == labels).sum().item()\n            total += len(labels)\n            \n            del logits, loss, narr_reprs, back_reprs\n            pbar.set_postfix({'acc': correct/total})\n            \n        # Validation\n        if val_loader:\n            classifier.eval()\n            v_correct, v_total = 0, 0\n            with torch.no_grad():\n                for batch in tqdm(val_loader, desc=\"Val\"):\n                    labels = batch['label'].to(device)\n                    narr_reprs = []\n                    for narr in batch['narrative']:\n                        chunks, _ = narrative_processor.process_narrative(narr)\n                        if len(chunks)>200: chunks = chunks[:200]\n                        # FIX: Squeeze here too\n                        c_reps = [model(input_ids=c.to(device)[:,:2048], output_hidden_states=True).hidden_states[-1].mean(1).squeeze(0).detach().cpu() for c in chunks]\n                        narr_reprs.append(torch.stack(c_reps).mean(0).to(device) if c_reps else torch.zeros(model.config.hidden_size).to(device))\n                    \n                    back_reprs = [backstory_embedder.aggregate_claims(c) for c in batch['backstory_claims']]\n                    logits, _ = classifier(torch.stack(narr_reprs), torch.stack(back_reprs))\n                    v_correct += (logits.argmax(1)==labels).sum().item()\n                    v_total += len(labels)\n            \n            val_acc = v_correct/v_total\n            print(f\"Epoch {epoch+1}: Train Acc={correct/total:.3f}, Val Acc={val_acc:.3f}\")\n            if val_acc > best_acc and save_dir:\n                best_acc = val_acc\n                os.makedirs(save_dir, exist_ok=True)\n                torch.save(classifier.state_dict(), os.path.join(save_dir, 'best_classifier.pt'))\n                \n    return classifier, model\n\nprint(\"‚úÖ Training function patched!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T23:12:06.293970Z","iopub.execute_input":"2026-01-08T23:12:06.294196Z","iopub.status.idle":"2026-01-08T23:12:06.306703Z","shell.execute_reply.started":"2026-01-08T23:12:06.294182Z","shell.execute_reply":"2026-01-08T23:12:06.306233Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training function patched!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================\n# CELL 9: Train Consistency Classifier\n# ============================================\n\n# Initialize processors\nnarrative_processor = NarrativeProcessor(\n    tokenizer=tokenizer,\n    chunk_size=2048,\n    overlap_size=256\n)\nbackstory_parser = BackstoryParser()\n\n# Fix: Override process_narrative to ensure proper chunking\n_original_process = narrative_processor.process_narrative\ndef _fixed_process(text, *args, **kwargs):\n    # Tokenize and chunk manually\n    tokens = tokenizer(text, return_tensors='pt', truncation=False, max_length=None)['input_ids']\n    chunks = []\n    chunk_size = 2048\n    for i in range(0, tokens.shape[1], chunk_size - 256):  # overlap\n        chunk = tokens[:, i:i+chunk_size]\n        if chunk.shape[1] > 0:\n            chunks.append(chunk)\n    if not chunks:\n        chunks = [tokens[:, :512]]  # fallback\n    return chunks, None\nnarrative_processor.process_narrative = _fixed_process\n\n# Create consistency dataset\nprint(\"Creating consistency dataset...\")\nconsistency_dataset = ConsistencyDataset(\n    narratives=narratives,\n    backstories=backstories,\n    labels=labels,\n    tokenizer=tokenizer,\n    narrative_processor=narrative_processor,\n    backstory_parser=backstory_parser,\n    max_narrative_length=2048\n)\n\n# Split\ntrain_size = int(0.8 * len(consistency_dataset))\nval_size = len(consistency_dataset) - train_size\ntrain_cons_ds, val_cons_ds = random_split(consistency_dataset, [train_size, val_size])\nprint(f\"Classifier training: {len(train_cons_ds)} train, {len(val_cons_ds)} val\")\n\n# Train\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STARTING CLASSIFIER TRAINING\")\nprint(\"=\" * 60)\n\n# Quick fix - access base dataset from Subset\ntrain_cons_ds.dataset.tokenizer = tokenizer\ntrain_cons_ds.dataset.narrative_processor = narrative_processor\n\nclassifier, model = train_consistency_classifier(\n    model=model,\n    classifier=classifier,\n    train_dataset=train_cons_ds,\n    val_dataset=val_cons_ds,\n    num_epochs=10,\n    batch_size=4,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    device=device,\n    save_dir=str(CHECKPOINT_DIR / \"classifier\"),\n    use_bf16=CONFIG['use_bf16'],\n    freeze_bdh=False\n)\nprint(\"‚úÖ Classifier training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T23:12:09.543598Z","iopub.execute_input":"2026-01-08T23:12:09.544057Z","iopub.status.idle":"2026-01-09T00:05:57.624290Z","shell.execute_reply.started":"2026-01-08T23:12:09.544042Z","shell.execute_reply":"2026-01-09T00:05:57.623778Z"}},"outputs":[{"name":"stdout","text":"Creating consistency dataset...\nClassifier training: 64 train, 16 val\n\n============================================================\nSTARTING CLASSIFIER TRAINING\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:23<00:00, 16.46s/it, acc=0.562]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:03<00:00, 15.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Acc=0.562, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:23<00:00, 16.50s/it, acc=0.594]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:04<00:00, 16.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Acc=0.594, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:25<00:00, 16.57s/it, acc=0.609]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:03<00:00, 15.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Acc=0.609, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:24<00:00, 16.51s/it, acc=0.562]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:04<00:00, 16.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Acc=0.562, Val Acc=0.688\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:24<00:00, 16.55s/it, acc=0.641]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:03<00:00, 15.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Acc=0.641, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:20<00:00, 16.29s/it, acc=0.609]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:03<00:00, 15.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=0.609, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:17<00:00, 16.11s/it, acc=0.594]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:01<00:00, 15.47s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Acc=0.594, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:15<00:00, 15.98s/it, acc=0.641]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:01<00:00, 15.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Acc=0.641, Val Acc=0.500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:08<00:00, 15.54s/it, acc=0.609]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:02<00:00, 15.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Acc=0.609, Val Acc=0.750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:13<00:00, 15.83s/it, acc=0.625]\nVal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:02<00:00, 15.68s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Acc=0.625, Val Acc=0.750\n‚úÖ Classifier training complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================\n# CELL 10: Run Inference on Test Set (FIXED)\n# ============================================\n\nimport gc\nfrom tqdm import tqdm\n\n# Clear GPU memory from training\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Load best models\ncheckpoint_path = CHECKPOINT_DIR / \"classifier\" / \"best_classifier.pt\"\nif checkpoint_path.exists():\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    if isinstance(checkpoint, dict) and 'classifier_state_dict' in checkpoint:\n        classifier.load_state_dict(checkpoint['classifier_state_dict'])\n    else:\n        classifier.load_state_dict(checkpoint)\n    print(\"Loaded best checkpoint\")\n\nmodel.eval()\nclassifier.eval()\n\n# Load test data\ntest_data = prepare_test_data(test_df, books_dir, verbose=True)\n\nprint(f\"\\nProcessing {len(test_data)} test examples...\")\nresults = []\n\nfor example in tqdm(test_data, desc=\"Inference\"):\n    try:\n        if not example.get('narrative'):\n            results.append({'id': example['id'], 'Prediction': 0, 'Rationale': \"No narrative\"})\n            continue\n        \n        # Process narrative in chunks\n        narrative_processor.chunk_size = 512  # Smaller chunks to save memory\n        chunks, _ = narrative_processor.process_narrative(example['narrative'])\n        \n        if len(chunks) > 100:\n            chunks = chunks[:100]  # Limit chunks\n        \n        # Extract narrative representation\n        chunk_reprs = []\n        with torch.no_grad():\n            for chunk_ids in chunks:\n                chunk_ids = chunk_ids.to(device)\n                if chunk_ids.dim() == 1:\n                    chunk_ids = chunk_ids.unsqueeze(0)\n                if chunk_ids.shape[-1] > 2048:\n                    chunk_ids = chunk_ids[:, :2048]\n                \n                outputs = model(input_ids=chunk_ids, output_hidden_states=True)\n                hidden = outputs.hidden_states[-1]\n                rep = hidden.mean(dim=1).squeeze(0)\n                chunk_reprs.append(rep.cpu())\n                \n                del outputs, hidden\n                torch.cuda.empty_cache()\n        \n        if chunk_reprs:\n            narr_repr = torch.stack(chunk_reprs).max(dim=0)[0].unsqueeze(0).to(device)\n        else:\n            narr_repr = torch.zeros(1, model.config.hidden_size, device=device)\n        \n        # Extract backstory representation\n        back_tokens = tokenizer(example['backstory'], return_tensors='pt', \n                                truncation=True, max_length=512)['input_ids'].to(device)\n        with torch.no_grad():\n            back_out = model(input_ids=back_tokens, output_hidden_states=True)\n            back_repr = back_out.hidden_states[-1].mean(dim=1)\n        \n        # Classify\n        with torch.no_grad():\n            logits, probs = classifier(narr_repr, back_repr)\n            pred = logits.argmax(dim=-1).item()\n            conf = probs.max().item()\n        \n        results.append({\n            'id': example['id'],\n            'Prediction': pred,\n            'Rationale': f\"Confidence: {conf:.2f}\"\n        })\n        \n        del narr_repr, back_repr, logits, probs\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"Error {example['id']}: {e}\")\n        results.append({'id': example['id'], 'Prediction': 0, 'Rationale': f\"Error: {str(e)[:50]}\"})\n        torch.cuda.empty_cache()\n\n# Save results\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(WORK_DIR / \"results.csv\", index=False)\nprint(f\"\\n‚úÖ Saved {len(results)} results\")\nprint(results_df['Prediction'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:19:15.282460Z","iopub.execute_input":"2026-01-09T00:19:15.282597Z","iopub.status.idle":"2026-01-09T00:22:32.337004Z","shell.execute_reply.started":"2026-01-09T00:19:15.282585Z","shell.execute_reply":"2026-01-09T00:22:32.336521Z"}},"outputs":[{"name":"stdout","text":"Loaded best checkpoint\nLoaded 60 test examples\nUnique books: 2\n\nProcessing 60 test examples...\n","output_type":"stream"},{"name":"stderr","text":"Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [03:16<00:00,  3.28s/it]","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved 60 results\nPrediction\n1    60\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# # Quick fix - inspect the checkpoint keys\n# checkpoint = torch.load(checkpoint_path, map_location=device)\n# print(checkpoint.keys())  # See what keys exist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:12:44.819986Z","iopub.execute_input":"2026-01-09T00:12:44.820216Z","iopub.status.idle":"2026-01-09T00:12:44.826363Z","shell.execute_reply.started":"2026-01-09T00:12:44.820202Z","shell.execute_reply":"2026-01-09T00:12:44.825902Z"}},"outputs":[{"name":"stdout","text":"odict_keys(['classifier.0.weight', 'classifier.0.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.5.weight', 'classifier.5.bias', 'classifier.8.weight', 'classifier.8.bias', 'classifier.9.weight', 'classifier.9.bias', 'classifier.12.weight', 'classifier.12.bias'])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ============================================\n# CELL 11: Download Results\n# ============================================\n\n# Results are at /kaggle/working/results.csv\nprint(f\"\\n{'='*60}\")\nprint(\"DONE! Results saved to: /kaggle/working/results.csv\")\nprint(\"Download from Kaggle notebook output\")\nprint(f\"{'='*60}\")\n\n# Display first few results\nprint(\"\\nFirst 10 predictions:\")\nprint(results_df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:22:32.337542Z","iopub.execute_input":"2026-01-09T00:22:32.337673Z","iopub.status.idle":"2026-01-09T00:22:32.350613Z","shell.execute_reply.started":"2026-01-09T00:22:32.337656Z","shell.execute_reply":"2026-01-09T00:22:32.350170Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nDONE! Results saved to: /kaggle/working/results.csv\nDownload from Kaggle notebook output\n============================================================\n\nFirst 10 predictions:\n    id  Prediction         Rationale\n0   95           1  Confidence: 0.56\n1  136           1  Confidence: 0.55\n2   59           1  Confidence: 0.57\n3   60           1  Confidence: 0.57\n4  124           1  Confidence: 0.54\n5  111           1  Confidence: 0.53\n6  135           1  Confidence: 0.54\n7   27           1  Confidence: 0.58\n8  110           1  Confidence: 0.56\n9   42           1  Confidence: 0.57\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# That comment is a note from the original BDH authors about potential numerical instability in linear attention. They're saying the recurrent sum computation could overflow/underflow for very long sequences. It's a known limitation, not something broken - just a caveat for extreme use cases.\n\n# About the loss (9.4) - it's actually not terrible for pretraining! Here's context:\n\n# Loss ~9.4 ‚âà perplexity ~12,000 - sounds bad but...\n# You're training a small model (20M params) on huge narratives (800K-2.6M chars each)\n# Only 72 training samples for 3 epochs\n# The vocab is ~30K tokens, so random guessing would give loss ~10.3\n# The loss dropped (9.4 ‚Üí 9.3 val) which means it IS learning.\n\n# To improve:\n\n# Train more epochs (10-20)\n# Use smaller sequence chunks in \n# NarrativeDataset\n#  (512 instead of 2048)\n# More data would help most\n# But for your task (consistency classification), the pretraining loss doesn't need to be great - you just need the model to learn narrative representations. The classifier training is what matters most for accuracy on the final task.\n\n# Continue to classifier training and see how the classification accuracy looks - that's your actual metric.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:05:57.648107Z","iopub.status.idle":"2026-01-09T00:05:57.648268Z","shell.execute_reply.started":"2026-01-09T00:05:57.648182Z","shell.execute_reply":"2026-01-09T00:05:57.648188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}